{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH1i0fLwbBvk"
      },
      "source": [
        "論文  \n",
        "https://arxiv.org/abs/2201.13433  \n",
        "  \n",
        "GitHub  \n",
        "https://github.com/yuval-alaluf/stylegan3-editing  \n",
        "  \n",
        "<a href=\"https://colab.research.google.com/github/kaz12tech/ai_demos/blob/master/StyleGAN3_demo.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs5lgHy1bBvq"
      },
      "source": [
        "# ランタイムの設定\n",
        "「ランタイム」→「ランタイムのタイプを変更」→「ハードウェアアクセラレータ」をGPUに変更"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiaCriA8bBvr"
      },
      "source": [
        "# 実行方法\n",
        "「ランタイム」→「すべてのセルを実行」を選択"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMxw_RANbBvr"
      },
      "source": [
        "# GPU確認"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UgMBgucbBvs"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOmsOtenbBvt"
      },
      "source": [
        "# 環境セットアップ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw-C-iMzbBvu"
      },
      "source": [
        "## ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t-0NeEYbBvv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxZf2yrrbBvv"
      },
      "source": [
        "## GitHubからコードを取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3sEvKlpbBvw"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "CODE_DIR = 'stylegan3-editing'\n",
        "!git clone https://github.com/yuval-alaluf/stylegan3-editing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EESUczUbBvx"
      },
      "source": [
        "## ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8VuMjGXbBvx"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "!pip install pyrallis\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o1gy8UebXhp"
      },
      "source": [
        "## ライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEJNmH-vbBvy"
      },
      "outputs": [],
      "source": [
        "%cd /content/{CODE_DIR}\n",
        "\n",
        "import time\n",
        "import sys\n",
        "import pprint\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import dataclasses\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from editing.interfacegan.face_editor import FaceEditor\n",
        "from editing.styleclip_global_directions import edit as styleclip_edit\n",
        "from models.stylegan3.model import GeneratorType\n",
        "from notebooks.notebook_utils import Downloader, ENCODER_PATHS, INTERFACEGAN_PATHS, STYLECLIP_PATHS\n",
        "from notebooks.notebook_utils import run_alignment, crop_image, compute_transforms\n",
        "from utils.common import tensor2im\n",
        "from utils.inference_utils import run_on_batch, load_encoder, get_average_image\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFnhM7Qibndh"
      },
      "source": [
        "# 学習済みモデルのセットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrfLYhGObqbc"
      },
      "outputs": [],
      "source": [
        "## Downloaderの設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LSppCHmbaKy"
      },
      "outputs": [],
      "source": [
        "download_with_pydrive = False #@param {type:\"boolean\"}\n",
        "downloader = Downloader(code_dir=CODE_DIR,\n",
        "                        use_pydrive=download_with_pydrive,\n",
        "                        subdir=\"pretrained_models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbU3rUHIbvLB"
      },
      "outputs": [],
      "source": [
        "#@markdown 学習済みモデルの選択\n",
        "experiment_type = 'restyle_pSp_ffhq' #@param ['restyle_e4e_ffhq', 'restyle_pSp_ffhq']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlagWhlPcqgG"
      },
      "source": [
        "## 推論時パラメータ設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkIex9zOcpem"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_DATA_ARGS = {\n",
        "    \"restyle_pSp_ffhq\": {\n",
        "        \"model_path\": \"./pretrained_models/restyle_pSp_ffhq.pt\",\n",
        "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    },\n",
        "    \"restyle_e4e_ffhq\": {\n",
        "        \"model_path\": \"./pretrained_models/restyle_e4e_ffhq.pt\",\n",
        "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    }\n",
        "}\n",
        "\n",
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jENX4t6sc-nZ"
      },
      "source": [
        "## 学習済みモデルのダウンロード\n",
        "/content/stylegan3-editing/pretrained_models/restyle_pSp_ffhq.pt  をダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNkUU7XcdBD0"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(EXPERIMENT_ARGS['model_path']) or os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
        "    print(f'Downloading ReStyle encoder model: {experiment_type}...')\n",
        "    try:\n",
        "      downloader.download_file(file_id=ENCODER_PATHS[experiment_type]['id'],\n",
        "                              file_name=ENCODER_PATHS[experiment_type]['name'])\n",
        "    except Exception as e:\n",
        "      raise ValueError(f\"Unable to download model correctly! {e}\")\n",
        "    # if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\n",
        "    if os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
        "        raise ValueError(\"Pretrained model was unable to be downloaded correctly!\")\n",
        "    else:\n",
        "        print('Done.')\n",
        "else:\n",
        "    print(f'Model for {experiment_type} already exists!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd3eS5kTdS7Q"
      },
      "source": [
        "## 学習済みモデルのロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UfIasuIdWjG"
      },
      "outputs": [],
      "source": [
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "net, opts = load_encoder(checkpoint_path=model_path)\n",
        "pprint.pprint(dataclasses.asdict(opts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XimE7l1VdxK5"
      },
      "source": [
        "# テスト画像の取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy6JOZ6Wd1BR"
      },
      "outputs": [],
      "source": [
        "%cd /content/{CODE_DIR}\n",
        "\n",
        "!mkdir upload_images\n",
        "\n",
        "# 画像ダウンロード\n",
        "!wget -c https://user0514.cdnw.net/shared/img/thumb/kuchikomi725_TP_V4.jpg \\\n",
        "      -O ./upload_images/test_img.jpg\n",
        "\n",
        "# 正方形に切り抜き\n",
        "img = cv2.imread(\"./upload_images/test_img.jpg\")\n",
        "cropped_image = img[8:264, 374:630]\n",
        "cv2.imwrite('./upload_images/cropped_img.jpg', cropped_image)\n",
        "image_path = Path('./upload_images/cropped_img.jpg')\n",
        "\n",
        "# repositoryの画像を使用する場合は以下\n",
        "#image_path = Path(EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"]) # ウィル・スミス\n",
        "\n",
        "original_image = Image.open(image_path).convert(\"RGB\")\n",
        "original_image = original_image.resize((256, 256))\n",
        "original_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UUwBBldhmGj"
      },
      "source": [
        "# 顔画像の下処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jcUsyE9hFER"
      },
      "source": [
        "## 顔部分の整列(alignment)、クロップ(crop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV6BA7fvgcMx"
      },
      "outputs": [],
      "source": [
        "%cd /content/{CODE_DIR}\n",
        "\n",
        "import dlib\n",
        "from utils.alignment_utils import align_face\n",
        "\n",
        "\n",
        "if not os.path.exists(\"./shape_predictor_68_face_landmarks.dat\"):\n",
        "  !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "  !bzip2 -d shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "predictor = dlib.shape_predictor(\"./shape_predictor_68_face_landmarks.dat\")\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "print(\"Aligning image...\")\n",
        "input_image = align_face(filepath=str(image_path), detector=detector, predictor=predictor)\n",
        "\n",
        "#input_image = run_alignment(image_path)\n",
        "cropped_image = crop_image(image_path)\n",
        "joined = np.concatenate([input_image.resize((256, 256)), cropped_image.resize((256, 256))], axis=1)\n",
        "Image.fromarray(joined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuGWEsZVhdv3"
      },
      "source": [
        "## LandmarkベースのTransform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4Wjh2F8hclh"
      },
      "outputs": [],
      "source": [
        "images_dir = Path(\"./images\")\n",
        "images_dir.mkdir(exist_ok=True, parents=True)\n",
        "cropped_path = images_dir / f\"cropped_{image_path.name}\"\n",
        "aligned_path = images_dir / f\"aligned_{image_path.name}\"\n",
        "cropped_image.save(cropped_path)\n",
        "input_image.save(aligned_path)\n",
        "landmarks_transform = compute_transforms(aligned_path=aligned_path, cropped_path=cropped_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQcgxHHBqQdk"
      },
      "source": [
        "# Inversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPZgDi8dqUeC"
      },
      "outputs": [],
      "source": [
        "n_iters_per_batch =  10#@param {type:\"integer\"}\n",
        "opts.n_iters_per_batch = n_iters_per_batch\n",
        "opts.resize_outputs = False  # generate outputs at full resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAPn9NMRqZg1"
      },
      "outputs": [],
      "source": [
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "transformed_image = img_transforms(input_image)\n",
        "\n",
        "avg_image = get_average_image(net)\n",
        "\n",
        "with torch.no_grad():\n",
        "    tic = time.time()\n",
        "    result_batch, result_latents = run_on_batch(inputs=transformed_image.unsqueeze(0).cuda().float(),\n",
        "                                                net=net,\n",
        "                                                opts=opts,\n",
        "                                                avg_image=avg_image,\n",
        "                                                landmarks_transform=torch.from_numpy(landmarks_transform).cuda().float())\n",
        "    toc = time.time()\n",
        "    print('Inference took {:.4f} seconds.'.format(toc - tic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4uS8WEbqm9w"
      },
      "outputs": [],
      "source": [
        "def get_coupled_results(result_batch, cropped_image):\n",
        "    result_tensors = result_batch[0]  # there's one image in our batch\n",
        "    resize_amount = (256, 256) if opts.resize_outputs else (opts.output_size, opts.output_size)\n",
        "    final_rec = tensor2im(result_tensors[-1]).resize(resize_amount)\n",
        "    input_im = cropped_image.resize(resize_amount)\n",
        "    res = np.concatenate([np.array(input_im), np.array(final_rec)], axis=1)\n",
        "    res = Image.fromarray(res)\n",
        "    return res\n",
        "\n",
        "res = get_coupled_results(result_batch, cropped_image)\n",
        "res.resize((1024, 512))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t22ijpE0qvQx"
      },
      "source": [
        "# 編集\n",
        "Inversionで得られた潜在コードを編集"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHUkoAyhrJOQ"
      },
      "source": [
        "## boundaries, styleclipのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIjs7kBrrAq-"
      },
      "outputs": [],
      "source": [
        "download_with_pydrive = False #@param {type:\"boolean\"}\n",
        "\n",
        "# download files for interfacegan\n",
        "downloader = Downloader(code_dir=CODE_DIR,\n",
        "                        use_pydrive=download_with_pydrive,\n",
        "                        subdir=\"editing/interfacegan/boundaries/ffhq\")\n",
        "print(\"Downloading InterFaceGAN boundaries...\")\n",
        "for editing_file, params in INTERFACEGAN_PATHS.items():\n",
        "    print(f\"Downloading {editing_file} boundary...\")\n",
        "    downloader.download_file(file_id=params['id'],\n",
        "                             file_name=params['name'])\n",
        "\n",
        "# download files for styleclip\n",
        "downloader = Downloader(code_dir=CODE_DIR,\n",
        "                        use_pydrive=download_with_pydrive,\n",
        "                        subdir=\"editing/styleclip_global_directions/sg3-r-ffhq-1024\")\n",
        "print(\"Downloading StyleCLIP auxiliary files...\")\n",
        "for editing_file, params in STYLECLIP_PATHS.items():\n",
        "    print(f\"Downloading {editing_file}...\")\n",
        "    downloader.download_file(file_id=params['id'],\n",
        "                             file_name=params['name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_20_NRPrp5G"
      },
      "source": [
        "## 編集パラメータ設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwJ_XQWerVnl"
      },
      "outputs": [],
      "source": [
        "editor = FaceEditor(stylegan_generator=net.decoder, generator_type=GeneratorType.ALIGNED)\n",
        "\n",
        "#@markdown 編集パラメータ設定\n",
        "edit_direction = 'age' #@param ['age', 'smile', 'pose', 'Male']\n",
        "min_value = -10 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
        "max_value = 10 #@param {type:\"slider\", min:-10, max:10, step:1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z13ipvYfroX1"
      },
      "source": [
        "## 編集実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPZpVLfMrgG_"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "print(f\"Performing edit for {edit_direction}...\")\n",
        "input_latent = torch.from_numpy(result_latents[0][-1]).unsqueeze(0).cuda()\n",
        "edit_images, edit_latents = editor.edit(latents=input_latent,\n",
        "                                        direction=edit_direction,\n",
        "                                        factor_range=(min_value, max_value),\n",
        "                                        user_transforms=landmarks_transform,\n",
        "                                        apply_user_transformations=True)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN08rJNbrvLE"
      },
      "source": [
        "## 編集結果表示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai9pXe6FrxHo"
      },
      "outputs": [],
      "source": [
        "def prepare_edited_result(edit_images):\n",
        "  if type(edit_images[0]) == list:\n",
        "      edit_images = [image[0] for image in edit_images]\n",
        "  res = np.array(edit_images[0].resize((512, 512)))\n",
        "  for image in edit_images[1:]:\n",
        "      res = np.concatenate([res, image.resize((512, 512))], axis=1)\n",
        "  res = Image.fromarray(res).convert(\"RGB\")\n",
        "  return res\n",
        "\n",
        "res = prepare_edited_result(edit_images)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuE15upilJxG"
      },
      "source": [
        "## 動画にまとめる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQCrLARglNV8"
      },
      "outputs": [],
      "source": [
        "%cd /content/{CODE_DIR}\n",
        "!rm -rf videos\n",
        "!mkdir -p videos/frames\n",
        "\n",
        "def create_video(edit_images):\n",
        "  if type(edit_images[0]) == list:\n",
        "    edit_images = [image[0] for image in edit_images]\n",
        "  res = np.array(edit_images[0].resize((512, 512)))\n",
        "  for i, image in enumerate( edit_images[1:] ):\n",
        "    work_dir = \"/content/\" + CODE_DIR\n",
        "    save_dir = os.path.join(work_dir, \"videos/frames\")\n",
        "    filename = edit_direction + \"_\" + str(i) + \".png\"\n",
        "    filename = os.path.join(save_dir, filename)\n",
        "    resize_img = image.resize((512, 512))\n",
        "    resize_img.save(filename)\n",
        "\n",
        "  framename = edit_direction + \"_%d.png\"\n",
        "  framename = os.path.join(save_dir, framename)\n",
        "  dst_video = os.path.join(work_dir, \"videos/result.mp4\")\n",
        "  !!ffmpeg -i {framename} -c:v libx264 -vf \"fps=25,format=yuv420p\" {dst_video}\n",
        "  return dst_video\n",
        "\n",
        "dst_video = create_video(edit_images)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open(dst_video, 'rb').read()\n",
        "data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=\"50%\" height=\"50%\" controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\"\"\")"
      ],
      "metadata": {
        "id": "XPQVN_aLosPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1JLS9MWs0Sy"
      },
      "source": [
        "# StyleCLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_fP9Aw6s1of"
      },
      "outputs": [],
      "source": [
        "styleclip_args = styleclip_edit.EditConfig()\n",
        "global_direction_calculator = styleclip_edit.load_direction_calculator(stylegan_model=net.decoder, opts=styleclip_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgyB1SfTs65C"
      },
      "source": [
        "## 編集パラメータ設定\n",
        "neutral_textはデフォルトが望ましい"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMBh9-Qvs5-j"
      },
      "outputs": [],
      "source": [
        "neutral_text = \"a face\" #@param {type:\"raw\"}\n",
        "target_text = \"a smiling face\" #@param {type:\"raw\"}\n",
        "alpha = 4 #@param {type:\"slider\", min:-5, max:5, step:0.5}\n",
        "beta = 0.13 #@param {type:\"slider\", min:-1, max:1, step:0.1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqVIGgF-tHAj"
      },
      "source": [
        "## 編集実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYLRiO02tGKq"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "opts = styleclip_edit.EditConfig()\n",
        "opts.alpha_min = alpha\n",
        "opts.alpha_max = alpha\n",
        "opts.num_alphas = 1\n",
        "opts.beta_min = beta\n",
        "opts.beta_max = beta\n",
        "opts.num_betas = 1\n",
        "opts.neutral_text = neutral_text\n",
        "opts.target_text = target_text\n",
        "\n",
        "input_latent = result_latents[0][-1]\n",
        "input_transforms = torch.from_numpy(landmarks_transform).cpu().numpy()\n",
        "print(f'Performing edit for: \"{opts.target_text}\"...')\n",
        "edit_res, edit_latent = styleclip_edit.edit_image(latent=input_latent,\n",
        "                                                  landmarks_transform=input_transforms,\n",
        "                                                  stylegan_model=net.decoder,\n",
        "                                                  global_direction_calculator=global_direction_calculator,\n",
        "                                                  opts=opts,\n",
        "                                                  image_name=None,\n",
        "                                                  save=False)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vwq61Xa3tN4F"
      },
      "outputs": [],
      "source": [
        "input_im = tensor2im(transformed_image).resize((512, 512))\n",
        "edited_im = tensor2im(edit_res[0]).resize((512, 512))\n",
        "edit_coupled = np.concatenate([np.array(input_im), np.array(edited_im)], axis=1)\n",
        "edit_coupled = Image.fromarray(edit_coupled)\n",
        "edit_coupled.resize((1024, 512))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}