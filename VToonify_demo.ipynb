{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze17Whr66OiC"
      },
      "source": [
        "論文<br>\n",
        "https://arxiv.org/abs/2209.11224<br>\n",
        "<br>\n",
        "GitHub<br>\n",
        "https://github.com/williamyang1991/VToonify<br>\n",
        "<br>\n",
        "<a href=\"https://colab.research.google.com/github/kaz12tech/ai_demos/blob/master/VToonify_demo.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOLKpkBa6OiI"
      },
      "source": [
        "# 環境セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlTYqHuHNHI3",
        "outputId": "6a591d9b-30ef-4029-8344-bb92803a2a89"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz8gTEgP6OiJ"
      },
      "source": [
        "## GitHubからソースコードを取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO7I9ozfFpmW",
        "outputId": "c599b0dc-65bb-4571-cbc9-da335ab80abb"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "!git clone https://github.com/williamyang1991/VToonify.git\n",
        "\n",
        "\n",
        "# Commits on Sep 23, 2022\n",
        "%cd /content/VToonify\n",
        "!git checkout 920b56478835873169b31dd3d134d29e7e16f94b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07hg4xM8tYCV"
      },
      "source": [
        "## ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRHm5XmrGGNB",
        "outputId": "e20347af-cbd4-4143-ef27-54265cc8a831"
      },
      "outputs": [],
      "source": [
        "%cd /content/VToonify\n",
        "\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force \n",
        "!pip install wget\n",
        "!pip install --upgrade gdown\n",
        "\n",
        "!pip install moviepy==0.2.3.5 imageio==2.4.1\n",
        "!pip install yt-dlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6sssZcc9CBF"
      },
      "source": [
        "## ライブラリをインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeUH114h9AEa",
        "outputId": "30776208-6a36-487d-8656-c4819e272b29"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import cv2\n",
        "import dlib\n",
        "import bz2\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from model.vtoonify import VToonify\n",
        "from model.bisenet.model import BiSeNet\n",
        "from model.encoder.align_all_parallel import align_face\n",
        "from util import save_image, load_image, visualize, load_psp_standalone, get_video_crop_parameter, tensor2cv2\n",
        "\n",
        "from yt_dlp import YoutubeDL\n",
        "from moviepy.video.fx.resize import resize\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"using device is\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E2Tf5qE7EjH"
      },
      "source": [
        "# 学習済みモデルのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHJjZa2Z7Juh",
        "outputId": "4d001874-b252-414e-b92b-027dd297712b"
      },
      "outputs": [],
      "source": [
        "%cd /content/VToonify\n",
        "\n",
        "!mkdir ckpts\n",
        "\n",
        "encoder_path = 'ckpts/encoder.pt'\n",
        "if not os.path.exists(encoder_path):\n",
        "  !gdown https://drive.google.com/uc?id=1NgI4mPkboYvYw3MWcdUaQhkr0OWgs9ej \\\n",
        "         -O {encoder_path}\n",
        "\n",
        "faceparsing_path = 'ckpts/faceparsing.pth'\n",
        "if not os.path.exists(faceparsing_path):\n",
        "  !gdown https://drive.google.com/uc?id=1jY0mTjVB8njDh6e0LP_2UxuRK3MnjoIR \\\n",
        "         -O {faceparsing_path}\n",
        "\n",
        "# cartoon_exstyle\n",
        "exstyle_path = 'ckpts/exstyle_code.npy'\n",
        "if not os.path.exists(exstyle_path):\n",
        "  !gdown https://drive.google.com/uc?id=1BuCeLk3ASZcoHlbfT28qNru4r5f-hErr \\\n",
        "         -O {exstyle_path}\n",
        "\n",
        "# cartoon026\n",
        "style_type = 'cartoon026'\n",
        "generator_path = 'ckpts/generator.pt'\n",
        "if not os.path.exists(generator_path):\n",
        "  !gdown https://drive.google.com/uc?id=1YJYODh_vEyUrL0q02okjcicpJhdYY8An \\\n",
        "         -O {generator_path}\n",
        "\n",
        "landmark_path = 'ckpts/shape_predictor_68_face_landmarks.dat'\n",
        "if not os.path.exists(landmark_path):\n",
        "  !wget -c http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 \\\n",
        "        -O ckpts/shape_predictor_68_face_landmarks.dat.bz2\n",
        "  zipfile = bz2.BZ2File('ckpts/shape_predictor_68_face_landmarks.dat.bz2')\n",
        "  data = zipfile.read()\n",
        "  open(landmark_path, 'wb').write(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIzbyKVipyij"
      },
      "outputs": [],
      "source": [
        "# 下記エラー発生の場合は下記\n",
        "# Access denied with the following error:\n",
        "#  \tToo many users have viewed or downloaded this file recently. Please\n",
        "# \ttry accessing the file again later. If the file you are trying to\n",
        "# \taccess is particularly large or is shared with many people, it may\n",
        "# \ttake up to 24 hours to be able to view or download the file. If you\n",
        "# \tstill can't access a file after 24 hours, contact your domain\n",
        "# \tadministrator.\n",
        "\n",
        "# Google Driveで「ドライブへのショートカットを追加」後、下記実行\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !cp /content/drive/MyDrive/encoder.pt {encoder_path}\n",
        "# !cp /content/drive/MyDrive/exstyle_code.npy {exstyle_path}\n",
        "# !cp /content/drive/MyDrive/faceparsing.pth {faceparsing_path}\n",
        "# !cp /content/drive/MyDrive/vtoonify_s026_d0.5.pt {generator_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I7Q_1K3g4pa"
      },
      "source": [
        "# モデルのロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEyXJpCUg6Yf"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5,0.5,0.5]),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "acf5aee2646b4aeaacf39bf8a36beec8",
            "b307d0190622432383c068232c2f9486",
            "0ba1bc2a57ff4b3faf234a8de48a3107",
            "4daf28544c3641f484e346fc291bdafe",
            "59c2b5b56aee49e1a475781a22a16846",
            "bd4731f377274bc69cdd1f6ddd9c2e63",
            "5895d2eb01b6427191b280e8071cf39e",
            "1c7e9e094caf479c88c886507118fe12",
            "b9732db6bfa64211ad43e581197215a8",
            "b0b2e9b6bd2a4b98a36f72970207f776",
            "29c63248ffd549ad92574d95b8619fe1"
          ]
        },
        "id": "U7Xb1b9ghSWo",
        "outputId": "4312b9b3-6ff3-42c4-da53-4e0116a3dae6"
      },
      "outputs": [],
      "source": [
        "%cd /content/VToonify\n",
        "\n",
        "# load generator\n",
        "vtoonify = VToonify(backbone = 'dualstylegan')\n",
        "vtoonify.load_state_dict(torch.load(generator_path, map_location=lambda storage, loc: storage)['g_ema'])\n",
        "vtoonify.to(device)\n",
        "\n",
        "# load faceparsing\n",
        "parsingpredictor = BiSeNet(n_classes=19)\n",
        "parsingpredictor.load_state_dict(torch.load(faceparsing_path, map_location=lambda storage, loc: storage))\n",
        "parsingpredictor.to(device).eval()\n",
        "\n",
        "# load randmark predicter\n",
        "landmarkpredictor = dlib.shape_predictor(landmark_path)\n",
        "\n",
        "# load encoder\n",
        "pspencoder = load_psp_standalone(encoder_path, device)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jicsa1vwoxl-"
      },
      "outputs": [],
      "source": [
        "exstyles = np.load(exstyle_path, allow_pickle='TRUE').item()\n",
        "stylename = list(exstyles.keys())[int(style_type[-3:])]\n",
        "exstyle = torch.tensor(exstyles[stylename]).to(device)\n",
        "with torch.no_grad():  \n",
        "  exstyle = vtoonify.zplus2wplus(exstyle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT2zkOGzsMfh"
      },
      "source": [
        "# Image Toonification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "lx4KxPi-sL3m",
        "outputId": "007772d2-9378-475e-9527-5925153e95e7"
      },
      "outputs": [],
      "source": [
        "%cd /content/VToonify\n",
        "\n",
        "# image_path = './data/077436.jpg'\n",
        "!wget -c https://www.pakutaso.com/shared/img/thumb/SAYA160312500I9A3721_TP_V.jpg \\\n",
        "      -O ./data/test01.jpg\n",
        "image_path = './data/test01.jpg'\n",
        "\n",
        "original_image = load_image(image_path)\n",
        "\n",
        "visualize(original_image[0], 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "M2J53MkksVXU",
        "outputId": "fc567c2b-f5b1-46a3-cf18-e19a9bf48183"
      },
      "outputs": [],
      "source": [
        "frame = cv2.imread(image_path)\n",
        "frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "scale = 1\n",
        "kernel_1d = np.array([[0.125],[0.375],[0.375],[0.125]])\n",
        "# We detect the face in the image, and resize the image so that the eye distance is 64 pixels.\n",
        "# Centered on the eyes, we crop the image to almost 400x400 (based on args.padding).\n",
        "paras = get_video_crop_parameter(frame, landmarkpredictor, padding=[200,200,200,200])\n",
        "if paras is not None:\n",
        "  h,w,top,bottom,left,right,scale = paras\n",
        "  H, W = int(bottom-top), int(right-left)\n",
        "  # for HR image, we apply gaussian blur to it to avoid over-sharp stylization results\n",
        "  if scale <= 0.75:\n",
        "    frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
        "  if scale <= 0.375:\n",
        "    frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
        "  frame = cv2.resize(frame, (w, h))[top:bottom, left:right]\n",
        "  x = transform(frame).unsqueeze(dim=0).to(device)\n",
        "else:\n",
        "  print('no face detected!')\n",
        "\n",
        "visualize(x[0].cpu(), 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT0W5HsbsaL0",
        "outputId": "8c258728-e8b3-4655-96a8-709662a07961"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  I = align_face(frame, landmarkpredictor)\n",
        "  I = transform(I).unsqueeze(dim=0).to(device)\n",
        "  s_w = pspencoder(I)\n",
        "  s_w = vtoonify.zplus2wplus(s_w)\n",
        "  s_w[:,:7] = exstyle[:,:7]\n",
        "  # parsing network works best on 512x512 images, so we predict parsing maps on upsmapled frames\n",
        "  # followed by downsampling the parsing maps\n",
        "  x_p = F.interpolate(parsingpredictor(2*(F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)))[0], \n",
        "                      scale_factor=0.5, recompute_scale_factor=False).detach()\n",
        "  # we give parsing maps lower weight (1/16)\n",
        "  inputs = torch.cat((x, x_p/16.), dim=1)\n",
        "  # d_s has no effect when backbone is toonify\n",
        "  y_tilde = vtoonify(inputs, s_w.repeat(inputs.size(0), 1, 1), d_s = 0.5)        \n",
        "  y_tilde = torch.clamp(y_tilde, -1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "taHShC8ascIZ",
        "outputId": "b9a5c943-1c5f-4f19-d735-beb4418056c8"
      },
      "outputs": [],
      "source": [
        "visualize(y_tilde[0].cpu(), 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzs8fSQCpbHu"
      },
      "source": [
        "# Video Toonification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpVGWj7IpXWj"
      },
      "outputs": [],
      "source": [
        "video_url = 'https://www.youtube.com/watch?v=gkr57P0fwbI' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown 動画の切り抜き範囲(秒)を指定してください。\\\n",
        "#@markdown 30秒以上の場合OOM発生の可能性が高いため注意\n",
        "start_sec =  30#@param {type:\"integer\"}\n",
        "end_sec =  33#@param {type:\"integer\"}\n",
        "\n",
        "(start_pt, end_pt) = (start_sec, end_sec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJaZ-y-Sp2NS",
        "outputId": "71699ac9-e791-41c3-cf5f-02bb8a70358d"
      },
      "outputs": [],
      "source": [
        "%cd /content/VToonify\n",
        "\n",
        "!mkdir -p test_video/frames\n",
        "\n",
        "download_resolution = 720\n",
        "full_video_path = '/content/VToonify/data/full_video.mp4'\n",
        "input_clip_path = '/content/VToonify/data/clip_video.mp4'\n",
        "\n",
        "# 動画ダウンロード\n",
        "ydl_opts = {'format': f'best[height<={download_resolution}]', 'overwrites': True, 'outtmpl': full_video_path}\n",
        "with YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([video_url])\n",
        "\n",
        "# 指定区間切り抜き\n",
        "with VideoFileClip(full_video_path) as video:\n",
        "    subclip = video.subclip(start_pt, end_pt)\n",
        "    subclip.write_videofile(input_clip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXAfjeBgqE5W"
      },
      "outputs": [],
      "source": [
        "video_path = './data/clip_video.mp4'\n",
        "video_cap = cv2.VideoCapture(video_path)\n",
        "num = int(video_cap.get(7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "MKl-bcQlqJJS",
        "outputId": "89e579fb-fe1f-415d-e995-7859bc9e8169"
      },
      "outputs": [],
      "source": [
        "success, frame = video_cap.read()\n",
        "if success == False:\n",
        "    assert('load video frames error')\n",
        "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "visualize(transform(frame), 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iA07JQ8mqNYm"
      },
      "outputs": [],
      "source": [
        "scale = 1\n",
        "kernel_1d = np.array([[0.125],[0.375],[0.375],[0.125]])\n",
        "# We proprocess the video by detecting the face in the first frame, \n",
        "# and resizing the frame so that the eye distance is 64 pixels.\n",
        "# Centered on the eyes, we crop the first frame to almost 400x400 (based on args.padding).\n",
        "# All other frames use the same resizing and cropping parameters as the first frame.\n",
        "paras = get_video_crop_parameter(frame, landmarkpredictor, padding=[200,200,200,200])\n",
        "if paras is None:\n",
        "    print('no face detected!')\n",
        "else:\n",
        "    h,w,top,bottom,left,right,scale = paras\n",
        "    H, W = int(bottom-top), int(right-left)\n",
        "# for HR video, we apply gaussian blur to the frames to avoid flickers caused by bilinear downsampling\n",
        "# this can also prevent over-sharp stylization results. \n",
        "if scale <= 0.75:\n",
        "    frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
        "if scale <= 0.375:\n",
        "    frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
        "frame = cv2.resize(frame, (w, h))[top:bottom, left:right]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "HaJelqk5qPDg",
        "outputId": "11c90d87-ade1-49bb-e108-c4638294534b"
      },
      "outputs": [],
      "source": [
        "visualize(transform(frame), 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r62YmuqMqSht"
      },
      "outputs": [],
      "source": [
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "videoWriter = cv2.VideoWriter('./output/result.mp4', fourcc, video_cap.get(5), (4*W, 4*H))\n",
        "batch_size = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLehU0Hbqfpe",
        "outputId": "afc08bc3-cfa0-4acd-af17-4e6f87fab330"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    batch_frames = []\n",
        "    for i in tqdm(range(num)):\n",
        "        if i == 0:        \n",
        "            I = align_face(frame, landmarkpredictor)\n",
        "            I = transform(I).unsqueeze(dim=0).to(device)\n",
        "            s_w = pspencoder(I)\n",
        "            s_w = vtoonify.zplus2wplus(s_w)\n",
        "            s_w[:,:7] = exstyle[:,:7]\n",
        "        else:\n",
        "            success, frame = video_cap.read()\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            if scale <= 0.75:\n",
        "                frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
        "            if scale <= 0.375:\n",
        "                frame = cv2.sepFilter2D(frame, -1, kernel_1d, kernel_1d)\n",
        "            frame = cv2.resize(frame, (w, h))[top:bottom, left:right]\n",
        "\n",
        "        batch_frames += [transform(frame).unsqueeze(dim=0).to(device)]\n",
        "\n",
        "        if len(batch_frames) == batch_size or (i+1) == num:\n",
        "            x = torch.cat(batch_frames, dim=0)\n",
        "            batch_frames = []\n",
        "            # parsing network works best on 512x512 images, so we predict parsing maps on upsmapled frames\n",
        "            # followed by downsampling the parsing maps\n",
        "            x_p = F.interpolate(parsingpredictor(2*(F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)))[0], \n",
        "                            scale_factor=0.5, recompute_scale_factor=False).detach()\n",
        "            # we give parsing maps lower weight (1/16)\n",
        "            inputs = torch.cat((x, x_p/16.), dim=1)\n",
        "            # d_s has no effect when backbone is toonify\n",
        "            y_tilde = vtoonify(inputs, s_w.repeat(inputs.size(0), 1, 1), d_s = 0.5)       \n",
        "            y_tilde = torch.clamp(y_tilde, -1, 1)\n",
        "            for k in range(y_tilde.size(0)):\n",
        "                videoWriter.write(tensor2cv2(y_tilde[k].cpu()))\n",
        "videoWriter.release()\n",
        "video_cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "txIf4b61qmJO",
        "outputId": "bdf0c648-ef8a-49f3-a76e-c326824012e4"
      },
      "outputs": [],
      "source": [
        "clip = VideoFileClip('./output/result.mp4')\n",
        "clip = resize(clip, height=420)\n",
        "clip.ipython_display()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "69158ccfe43d0b962045f592ede11796dd42f250837ab62152c8bc6cd100a15b"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ba1bc2a57ff4b3faf234a8de48a3107": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c7e9e094caf479c88c886507118fe12",
            "max": 46827520,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9732db6bfa64211ad43e581197215a8",
            "value": 46827520
          }
        },
        "1c7e9e094caf479c88c886507118fe12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29c63248ffd549ad92574d95b8619fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4daf28544c3641f484e346fc291bdafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0b2e9b6bd2a4b98a36f72970207f776",
            "placeholder": "​",
            "style": "IPY_MODEL_29c63248ffd549ad92574d95b8619fe1",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 213MB/s]"
          }
        },
        "5895d2eb01b6427191b280e8071cf39e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59c2b5b56aee49e1a475781a22a16846": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acf5aee2646b4aeaacf39bf8a36beec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b307d0190622432383c068232c2f9486",
              "IPY_MODEL_0ba1bc2a57ff4b3faf234a8de48a3107",
              "IPY_MODEL_4daf28544c3641f484e346fc291bdafe"
            ],
            "layout": "IPY_MODEL_59c2b5b56aee49e1a475781a22a16846"
          }
        },
        "b0b2e9b6bd2a4b98a36f72970207f776": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b307d0190622432383c068232c2f9486": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd4731f377274bc69cdd1f6ddd9c2e63",
            "placeholder": "​",
            "style": "IPY_MODEL_5895d2eb01b6427191b280e8071cf39e",
            "value": "100%"
          }
        },
        "b9732db6bfa64211ad43e581197215a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd4731f377274bc69cdd1f6ddd9c2e63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
