{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze17Whr66OiC"
      },
      "source": [
        "論文<br>\n",
        "https://arxiv.org/abs/2209.09898<br>\n",
        "<br>\n",
        "GitHub<br>\n",
        "https://github.com/FrozenBurning/Text2Light<br>\n",
        "<br>\n",
        "<a href=\"https://colab.research.google.com/github/kaz12tech/ai_demos/blob/master/text2light_demo.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOLKpkBa6OiI"
      },
      "source": [
        "# 環境セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlTYqHuHNHI3",
        "outputId": "4bca99b9-e128-4ac1-815b-941db44f11cb"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz8gTEgP6OiJ"
      },
      "source": [
        "## GitHubからソースコードを取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO7I9ozfFpmW",
        "outputId": "0362f233-9fea-4a89-8d64-b2cbd9717778"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "!git clone https://github.com/FrozenBurning/Text2Light\n",
        "\n",
        "\n",
        "# Commits on Sep 22, 2022\n",
        "%cd /content/Text2Light\n",
        "!git checkout acd216150a76b8bb4e460801e95100d167a6f7fe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07hg4xM8tYCV"
      },
      "source": [
        "## ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gRHm5XmrGGNB",
        "outputId": "bc204e9b-450c-415a-deac-42c4ca3f7933"
      },
      "outputs": [],
      "source": [
        "%cd /content/Text2Light\n",
        "\n",
        "!sudo apt-get install libomp-dev\n",
        "\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning tensorboardX einops transformers\n",
        "!pip install kornia\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install faiss\n",
        "!pip install --upgrade gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6sssZcc9CBF"
      },
      "source": [
        "## ライブラリをインポート"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeUH114h9AEa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import argparse, os, sys, glob\n",
        "import cv2\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import clip\n",
        "from taming.util import instantiate_from_config\n",
        "from sritmo.global_sritmo import SRiTMO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E2Tf5qE7EjH"
      },
      "source": [
        "# 学習済みモデルのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHJjZa2Z7Juh",
        "outputId": "164323fe-f6fa-43ff-f41e-fd1b3c1664c7"
      },
      "outputs": [],
      "source": [
        "%cd /content/Text2Light\n",
        "!mkdir -p ckpts\n",
        "\n",
        "\n",
        "ckpt_path = 'ckpts/text2light_released_model/global_sampler_clip/checkpoints/last.ckpt'\n",
        "if not os.path.exists(ckpt_path):\n",
        "  !gdown --folder https://drive.google.com/drive/folders/1HKBjC7oQOzrkGFKMQmSh6PySv6AycDS3?usp=sharing \\\n",
        "         -O 'ckpts/text2light_released_model'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D192kl9rVG-Z",
        "outputId": "51a054cf-fe32-4697-fb00-da6ad9055ab6"
      },
      "outputs": [],
      "source": [
        "# 学習済みモデルのダウンロードで下記エラーが出た場合はこのセルを実行\n",
        "# Access denied with the following error:\n",
        "\n",
        "#  \tToo many users have viewed or downloaded this file recently. Please\n",
        "# \ttry accessing the file again later. If the file you are trying to\n",
        "# \taccess is particularly large or is shared with many people, it may\n",
        "# \ttake up to 24 hours to be able to view or download the file. If you\n",
        "# \tstill can't access a file after 24 hours, contact your domain\n",
        "# \tadministrator. \n",
        "\n",
        "# text2light_released_modelの「ドライブへのショートカットを追加」後\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!rm -rf /content/Text2Light/ckpts/text2light_released_model\n",
        "!cp -r /content/drive/MyDrive/text2light_released_model /content/Text2Light/ckpts/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2ZhM26IQcDI"
      },
      "source": [
        "# Utility関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhE2HP70R769"
      },
      "source": [
        "## save img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wf8ZcgOQbn_"
      },
      "outputs": [],
      "source": [
        "# Tensor imgの保存\n",
        "def save_image(x, path):\n",
        "  c,h,w = x.shape\n",
        "  assert c==3\n",
        "  x = ((x.detach().cpu().numpy().transpose(1,2,0)+1.0)*127.5).clip(0,255).astype(np.uint8)\n",
        "  s = Image.fromarray(x)\n",
        "  s.save(path)\n",
        "  return s\n",
        "\n",
        "def get_knn(database: np.array, index: faiss.Index, txt_emb, k = 5):\n",
        "  dist, idx  = index.search(txt_emb, k)\n",
        "  return database[idx], idx #[bs, k, 512]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxl_nEh2R6WF"
      },
      "source": [
        "## predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCJTG-4aQv_m"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def text2light(models: dict, prompts, outdir, params: dict):\n",
        "  # models\n",
        "  global_sampler = models[\"gs\"]\n",
        "  local_sampler = models[\"ls\"]\n",
        "  # params\n",
        "  batch_size = len(prompts)\n",
        "  top_k = params[\"top_k\"]\n",
        "  temperature = params['temperature']\n",
        "  database = params['data4knn']\n",
        "  faiss_index = params['index4knn']\n",
        "  device = params['device']\n",
        "\n",
        "  # embed input texts\n",
        "  lan_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "  lan_model.eval()\n",
        "  text = clip.tokenize(prompts).to(device)\n",
        "  text_features = lan_model.encode_text(text)\n",
        "  target_txt_emb = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "  cond, _ = get_knn(database, faiss_index, target_txt_emb.cpu().numpy().astype('float32'))\n",
        "  txt_cond = torch.from_numpy(cond.reshape(batch_size, 5, cond.shape[-1]))\n",
        "  txt_cond = torch.cat([txt_cond, txt_cond,], dim=-1).to(device)\n",
        "\n",
        "  # sample holistic condition\n",
        "  bs = batch_size\n",
        "  start = 0\n",
        "  idx = torch.zeros(bs, 1, dtype=int)[:, :start].to(device)\n",
        "  cshape = [bs, 256, 8, 16]\n",
        "  sample = True\n",
        "\n",
        "  print(\"Generating holistic conditions according to texts...\")\n",
        "  for i in tqdm(range(start, cshape[2]*cshape[3])):\n",
        "    logits, _ = global_sampler.transformer(idx, embeddings=txt_cond)\n",
        "    logits = logits[:, -1, :]\n",
        "    if top_k is not None:\n",
        "      logits = global_sampler.top_k_logits(logits, top_k)\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    if sample:\n",
        "      ix = torch.multinomial(probs, num_samples=1)\n",
        "    else:\n",
        "      _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "    idx = torch.cat((idx, ix), dim=1)\n",
        "\n",
        "  xsample_holistic = global_sampler.decode_to_img(idx, cshape)\n",
        "  for i in range(xsample_holistic.shape[0]):\n",
        "    holistic_save = save_image(xsample_holistic[i], os.path.join(outdir, \"holistic\", \"holistic_[{}].png\".format(prompts[i])))\n",
        "\n",
        "  print(\"Synthesizing patches...\")\n",
        "  # synthesize patch by patch according to holistic condition\n",
        "  h = 512\n",
        "  w = 1024\n",
        "  xx, yy = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n",
        "  screen_points = np.stack([xx, yy], axis=-1)\n",
        "  coord = (screen_points * 2 - 1) * np.array([np.pi, np.pi/2])\n",
        "  spe = torch.from_numpy(coord).to(xsample_holistic).repeat(xsample_holistic.shape[0], 1, 1, 1).permute(0, 3, 1, 2)\n",
        "  spe = torch.nn.functional.interpolate(spe, scale_factor=1/8, mode=\"bicubic\", recompute_scale_factor=False, align_corners=True)\n",
        "  spe = local_sampler.embedder(spe.permute(0, 2, 3, 1))\n",
        "  spe = spe.permute(0, 3, 1, 2)\n",
        "\n",
        "  _, h_indices = local_sampler.encode_to_h(xsample_holistic)\n",
        "  cshape = [xsample_holistic.shape[0], 256, h // 16, w // 16]\n",
        "  idx = torch.randint(0, 1024, (cshape[0], cshape[2], cshape[3])).to(h_indices)\n",
        "  idx = idx.reshape(cshape[0], cshape[2], cshape[3])\n",
        "  \n",
        "  start = 0\n",
        "  start_i = start // cshape[3]\n",
        "  start_j = start % cshape[3]\n",
        "  sample = True\n",
        "\n",
        "  for i in tqdm(range(start_i, cshape[2])):\n",
        "    if i <= 8:\n",
        "      local_i = i\n",
        "    elif cshape[2]-i < 8:\n",
        "      local_i = 16-(cshape[2]-i)\n",
        "    else:\n",
        "      local_i = 8\n",
        "    for j in range(start_j, cshape[3]):\n",
        "      if j <= 8:\n",
        "        local_j = j\n",
        "      elif cshape[3]-j < 8:\n",
        "        local_j = 16-(cshape[3]-j)\n",
        "      else:\n",
        "        local_j = 8\n",
        "\n",
        "        i_start = i-local_i\n",
        "        i_end = i_start+16\n",
        "        j_start = j-local_j\n",
        "        j_end = j_start+16\n",
        "        patch = idx[:,i_start:i_end,j_start:j_end]\n",
        "        patch = patch.reshape(patch.shape[0],-1)\n",
        "        cpatch = spe[:, :, i_start*2:i_end*2,j_start*2:j_end*2]\n",
        "        cpatch = cpatch.reshape(cpatch.shape[0], local_sampler.cdim, -1)\n",
        "        patch = torch.cat((h_indices, patch), dim=1)\n",
        "        logits, _ = local_sampler.transformer(patch[:,:-1], embeddings=cpatch)\n",
        "        logits = logits[:, -256:, :]\n",
        "        logits = logits.reshape(cshape[0],16,16,-1)\n",
        "        logits = logits[:,local_i,local_j,:]\n",
        "        logits = logits / temperature\n",
        "\n",
        "        if top_k is not None:\n",
        "          logits = local_sampler.top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "          ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "          _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        idx[:,i,j] = ix.reshape(-1)\n",
        "  xsample = local_sampler.decode_to_img(idx, cshape)\n",
        "  for i in range(xsample.shape[0]):\n",
        "    ldr_save = save_image(xsample[i], os.path.join(outdir, \"ldr\", \"ldr_[{}].png\".format(prompts[i])))\n",
        "\n",
        "  # super-resolution inverse tone mapping\n",
        "  if params['sritmo'] is not None:\n",
        "    ldr_hr_samples, hdr_hr_samples = SRiTMO(xsample, params)\n",
        "  else:\n",
        "    print(\"no checkpoint provided, skip Stage II (SR-iTMO)...\")\n",
        "    return\n",
        "      \n",
        "  for i in range(xsample.shape[0]):\n",
        "    ldr_hr_save = (ldr_hr_samples[i].permute(1, 2, 0).detach().cpu().numpy() + 1) * 127.5\n",
        "    cv2.imwrite(os.path.join(outdir, \"ldr\", \"hrldr_[{}].png\".format(prompts[i])), ldr_hr_save)\n",
        "    # cv2.imwrite(os.path.join(outdir, \"hdr\", \"hdr_[{}].exr\".format(prompts[i])), hdr_hr_samples[i].permute(1, 2, 0).detach().cpu().numpy())\n",
        "  return ldr_hr_save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WimndFypR3Pw"
      },
      "source": [
        "## load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3XCavlYRhao"
      },
      "outputs": [],
      "source": [
        "def load_model_from_config(config, sd, gpu=True, eval_mode=True):\n",
        "  if \"ckpt_path\" in config.params:\n",
        "    print(\"Deleting the restore-ckpt path from the config...\")\n",
        "    config.params.ckpt_path = None\n",
        "  if \"downsample_cond_size\" in config.params:\n",
        "    print(\"Deleting downsample-cond-size from the config and setting factor=0.5 instead...\")\n",
        "    config.params.downsample_cond_size = -1\n",
        "    config.params[\"downsample_cond_factor\"] = 0.5\n",
        "  try:\n",
        "    if \"ckpt_path\" in config.params.first_stage_config.params:\n",
        "      config.params.first_stage_config.params.ckpt_path = None\n",
        "      print(\"Deleting the first-stage restore-ckpt path from the config...\")\n",
        "    if \"ckpt_path\" in config.params.cond_stage_config.params:\n",
        "      config.params.cond_stage_config.params.ckpt_path = None\n",
        "      print(\"Deleting the cond-stage restore-ckpt path from the config...\")\n",
        "    if \"ckpt_path\" in config.params.holistic_config.params:\n",
        "      config.params.holistic_config.params.ckpt_path = None\n",
        "      print(\"Deleting the global sampler restore-ckpt path from the config...\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  model = instantiate_from_config(config)\n",
        "  if sd is not None:\n",
        "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
        "    print(f\"Missing Keys in State Dict: {missing}\")\n",
        "    print(f\"Unexpected Keys in State Dict: {unexpected}\")\n",
        "  if gpu:\n",
        "    model.cuda()\n",
        "  if eval_mode:\n",
        "    model.eval()\n",
        "  return {\"model\": model}\n",
        "\n",
        "def load_model(config, ckpt, gpu, eval_mode):\n",
        "  if ckpt:\n",
        "    raw_model = torch.load(ckpt, map_location=\"cpu\")\n",
        "    state_dict = raw_model[\"state_dict\"]\n",
        "  else:\n",
        "    raise NotImplementedError(\"checkpoint at [{}] is not found!\".format(ckpt))\n",
        "  model = load_model_from_config(config.model, state_dict, gpu=gpu, eval_mode=eval_mode)[\"model\"]\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRyFyPmrR98h"
      },
      "source": [
        "# Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvZ2YMxQR__M"
      },
      "outputs": [],
      "source": [
        "texts = \"sunny park\" #@param {type:\"string\"}\n",
        "model = \"outdoor\" #@param [\"full\", \"outdoor\", \"indoor\"]\n",
        "sritmo = True #@param {type:\"boolean\"}\n",
        "sr_factor = 4 #@param {type:\"number\"}\n",
        "top_k = 300 #@param {type:\"number\"}\n",
        "temperature = 1.0 #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iGvJ5rUSUR-"
      },
      "outputs": [],
      "source": [
        "local_sampler_path = None\n",
        "if model == \"full\":\n",
        "  local_sampler_path = \"./ckpts/text2light_released_model/local_sampler/\"\n",
        "elif model == \"outdoor\":\n",
        "  local_sampler_path = \"./ckpts/text2light_released_model/local_sampler_outdoor/\"\n",
        "elif model == \"indoor\":\n",
        "  local_sampler_path = \"./ckpts/text2light_released_model/local_sampler_indoor/\"\n",
        "else:\n",
        "  raise NotImplementedError\n",
        "\n",
        "sritmo_path = None\n",
        "if sritmo:\n",
        "  sritmo_path = \"./ckpts/text2light_released_model/sritmo.pth\"\n",
        "\n",
        "\n",
        "opt = argparse.Namespace(\n",
        "    resume_global=\"./ckpts/text2light_released_model/global_sampler_clip/\",\n",
        "    resume_local=local_sampler_path,\n",
        "    sritmo=sritmo_path,\n",
        "    sr_factor=sr_factor,\n",
        "    outdir=\"./text2light_generated\",\n",
        "    clip=\"./clip_emb.npy\",\n",
        "    text=texts,\n",
        "    top_k=top_k,\n",
        "    temperature=temperature,\n",
        "    bs=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEP0M2sHSdf2",
        "outputId": "a71817f7-ff52-405f-9313-8d079664c7e0"
      },
      "outputs": [],
      "source": [
        "sys.path.append(os.getcwd())\n",
        "unknown=[]\n",
        "gpu = True\n",
        "eval_mode = True\n",
        "show_config = False\n",
        "\n",
        "base = list()\n",
        "\n",
        "ckpt = None\n",
        "if opt.resume_global:\n",
        "  if not os.path.exists(opt.resume_global):\n",
        "    raise ValueError(\"Cannot find {}\".format(opt.resume_global))\n",
        "  print(\"Resuming from global sampler ckpt...\")\n",
        "  assert os.path.isdir(opt.resume_global), opt.resume_global\n",
        "  logdir = opt.resume_global.rstrip(\"/\")\n",
        "  ckpt = os.path.join(logdir, \"checkpoints\", \"last.ckpt\")\n",
        "  print(f\"logdir:{logdir}\")\n",
        "  base_configs = sorted(glob.glob(os.path.join(logdir, \"configs/*-project.yaml\")))\n",
        "  config2load = base_configs + base\n",
        "\n",
        "configs = [OmegaConf.load(cfg) for cfg in config2load]\n",
        "cli = OmegaConf.from_dotlist(unknown)\n",
        "config = OmegaConf.merge(*configs, cli)\n",
        "\n",
        "print(ckpt)\n",
        "if show_config:\n",
        "  print(OmegaConf.to_container(config))\n",
        "\n",
        "global_sampler = load_model(config, ckpt, gpu, eval_mode)\n",
        "\n",
        "ckpt = None\n",
        "if opt.resume_local:\n",
        "  if not os.path.exists(opt.resume_local):\n",
        "    raise ValueError(\"Cannot find {}\".format(opt.resume_local))\n",
        "  print(\"Resuming from local sampler ckpt...\")\n",
        "  assert os.path.isdir(opt.resume_local), opt.resume_local\n",
        "  logdir = opt.resume_local.rstrip(\"/\")\n",
        "  ckpt = os.path.join(logdir, \"checkpoints\", \"last.ckpt\")\n",
        "  print(f\"logdir:{logdir}\")\n",
        "  base_configs = sorted(glob.glob(os.path.join(logdir, \"configs/*-project.yaml\")))\n",
        "  config2load = base_configs + base\n",
        "\n",
        "configs = [OmegaConf.load(cfg) for cfg in config2load]\n",
        "cli = OmegaConf.from_dotlist(unknown)\n",
        "config = OmegaConf.merge(*configs, cli)\n",
        "print(ckpt)\n",
        "if show_config:\n",
        "  print(OmegaConf.to_container(config))\n",
        "\n",
        "local_sampler = load_model(config, ckpt, gpu, eval_mode)\n",
        "\n",
        "outdir = opt.outdir\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "print(\"Writing samples to \", outdir)\n",
        "for k in [\"holistic\", \"ldr\", \"hdr\"]:\n",
        "  os.makedirs(os.path.join(outdir, k), exist_ok=True)\n",
        "    \n",
        "prompts_file = opt.text\n",
        "if os.path.exists(prompts_file):\n",
        "  # list of prompts for text2light tasks\n",
        "  with open(prompts_file, 'r') as f:\n",
        "    prompts = f.read().splitlines()\n",
        "else:\n",
        "  # a single prompt\n",
        "  prompts = [prompts_file]\n",
        "\n",
        "# construct knn searching base\n",
        "if os.path.isfile(opt.clip):\n",
        "  clip_emb = np.load(opt.clip).astype('float32')\n",
        "else:\n",
        "  raise NotImplementedError('The path [{}] to clip embedding is not valid.'.format(opt.clip))\n",
        "    \n",
        "knn_index = faiss.IndexFlatIP(clip_emb.shape[-1])\n",
        "knn_index.add(clip_emb)\n",
        "\n",
        "input_models = {\n",
        "  'gs': global_sampler,\n",
        "  'ls': local_sampler,\n",
        "}\n",
        "\n",
        "input_params = {\n",
        "  'top_k': opt.top_k,\n",
        "  'temperature': opt.temperature,\n",
        "  'device': 'cuda' if gpu else 'cpu',\n",
        "  'data4knn': clip_emb,\n",
        "  'index4knn': knn_index,\n",
        "  'sritmo': opt.sritmo,\n",
        "  'sr_factor': opt.sr_factor,\n",
        "}\n",
        "for i in range(0, len(prompts), opt.bs):\n",
        "  end_i = min(len(prompts), i + opt.bs)\n",
        "  prompt = prompts[i: i+opt.bs]\n",
        "  result = text2light(input_models, prompt, outdir, input_params)\n",
        "  display(Image.fromarray(result[:, :, [2, 1, 0]].clip(0,255).astype(np.uint8)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "69158ccfe43d0b962045f592ede11796dd42f250837ab62152c8bc6cd100a15b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
