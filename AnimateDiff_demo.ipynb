{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze17Whr66OiC"
      },
      "source": [
        "論文<br>\n",
        "https://arxiv.org/abs/2307.04725<br>\n",
        "<br>\n",
        "GitHub<br>\n",
        "https://github.com/guoyww/animatediff<br>\n",
        "<br>\n",
        "<a href=\"https://colab.research.google.com/github/kaz12tech/ai_demos/blob/master/AnimateDiff_demo.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOLKpkBa6OiI"
      },
      "source": [
        "# setup environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz8gTEgP6OiJ"
      },
      "source": [
        "## git clone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SO7I9ozfFpmW"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "!git clone https://github.com/guoyww/AnimateDiff.git\n",
        "\n",
        "%cd /content/AnimateDiff\n",
        "# Commits on Jul 21, 2023\n",
        "!git checkout 53c63ad8391d7095ab5364c13b4aa3a7d183dac5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07hg4xM8tYCV"
      },
      "source": [
        "## install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRHm5XmrGGNB"
      },
      "outputs": [],
      "source": [
        "%cd /content/AnimateDiff\n",
        "\n",
        "!pip install omegaconf einops omegaconf safetensors diffusers[torch]==0.11.1 transformers xformers==0.0.20 triton==2.0.0\n",
        "!pip install --upgrade gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E2Tf5qE7EjH"
      },
      "source": [
        "# download pretrain models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHJjZa2Z7Juh"
      },
      "outputs": [],
      "source": [
        "%cd /content/AnimateDiff\n",
        "\n",
        "# download StableDiffusion\n",
        "!rm -rf ./models/StableDiffusion/\n",
        "!git clone -b fp16 https://huggingface.co/runwayml/stable-diffusion-v1-5 ./models/StableDiffusion/stable-diffusion-v1-5\n",
        "\n",
        "# download base T2I\n",
        "!rm -rf ./models/Motion_Module/*\n",
        "# !bash download_bashscripts/0-MotionModule.sh\n",
        "!wget -c https://huggingface.co/camenduru/AnimateDiff/resolve/main/mm_sd_v14.ckpt \\\n",
        "      -O ./models/Motion_Module/mm_sd_v14.ckpt\n",
        "\n",
        "!wget -c https://huggingface.co/camenduru/AnimateDiff/resolve/main/mm_sd_v15.ckpt \\\n",
        "      -O ./models/Motion_Module/mm_sd_v15.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTJpNPCjEuJ8"
      },
      "outputs": [],
      "source": [
        "T2I_TYPE = \"1-ToonYou\" #@param [\"1-ToonYou\", \"2-Lyriel\", \"3-RcnzCartoon\", \"4-MajicMix\", \"5-RealisticVision\", \"6-Tusun\", \"7-FilmVelvia\", \"8-GhibliBackground\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivlskB8sEVCl"
      },
      "outputs": [],
      "source": [
        "!bash download_bashscripts/{T2I_TYPE}.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKlqBAqJFibk"
      },
      "source": [
        "# Create Animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeBgjGE4IpgI"
      },
      "outputs": [],
      "source": [
        "%cd /content/AnimateDiff\n",
        "\n",
        "# fix codes\n",
        "!sed -i ./animatediff/utils/convert_from_ckpt.py \\\n",
        "     -e \"s/load_state_dict(text_model_dict)/load_state_dict(text_model_dict, strict=False)/g\"\n",
        "\n",
        "\n",
        "!sed -i ./animatediff/utils/util.py \\\n",
        "     -e \"s/fps=fps/duration=1000\\/fps/g\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ofqffiUrUSO"
      },
      "outputs": [],
      "source": [
        "%cd /content/AnimateDiff\n",
        "\n",
        "import argparse\n",
        "import datetime\n",
        "import inspect\n",
        "import os\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "import torch\n",
        "\n",
        "import diffusers\n",
        "from diffusers import AutoencoderKL, DDIMScheduler\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "from animatediff.models.unet import UNet3DConditionModel\n",
        "from animatediff.pipelines.pipeline_animation import AnimationPipeline\n",
        "from animatediff.utils.util import save_videos_grid\n",
        "from animatediff.utils.convert_from_ckpt import convert_ldm_unet_checkpoint, convert_ldm_clip_checkpoint, convert_ldm_vae_checkpoint\n",
        "from animatediff.utils.convert_lora_safetensor_to_diffusers import convert_lora\n",
        "from diffusers.utils.import_utils import is_xformers_available\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "import csv, pdb, glob\n",
        "from safetensors import safe_open\n",
        "import math\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu8nQTTLFkPj"
      },
      "outputs": [],
      "source": [
        "%cd /content/AnimateDiff\n",
        "\n",
        "# set args\n",
        "args = argparse.ArgumentParser()\n",
        "args.pretrained_model_path = 'models/StableDiffusion/stable-diffusion-v1-5'\n",
        "args.inference_config = 'configs/inference/inference.yaml'\n",
        "args.config = f'configs/prompts/{T2I_TYPE}.yaml'\n",
        "args.L = 16\n",
        "args.W = 512\n",
        "args.H = 512\n",
        "args.prompt = 'cherry blossoms, woman, 4k, definition, colorful'\n",
        "args.n_prompt = ''\n",
        "args.random_seed = 12\n",
        "\n",
        "*_, func_args = inspect.getargvalues(inspect.currentframe())\n",
        "func_args = dict(func_args)\n",
        "\n",
        "time_str = datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "savedir = f\"samples/{Path(args.config).stem}-{time_str}\"\n",
        "os.makedirs(savedir)\n",
        "inference_config = OmegaConf.load(args.inference_config)\n",
        "\n",
        "config  = OmegaConf.load(args.config)\n",
        "samples = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rj5W-bXAaa6I"
      },
      "outputs": [],
      "source": [
        "sample_idx = 0\n",
        "for model_idx, (config_key, model_config) in enumerate(list(config.items())[:1]):\n",
        "  print(model_idx, (config_key, model_config))\n",
        "\n",
        "  motion_modules = model_config.motion_module\n",
        "  motion_modules = [motion_modules] if isinstance(motion_modules, str) else list(motion_modules)\n",
        "  for motion_module in motion_modules[:1]:\n",
        "    print(motion_module)\n",
        "\n",
        "    ### >>> create validation pipeline >>> ###\n",
        "    tokenizer    = CLIPTokenizer.from_pretrained(args.pretrained_model_path, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_path, subfolder=\"text_encoder\")\n",
        "    vae          = AutoencoderKL.from_pretrained(args.pretrained_model_path, subfolder=\"vae\")\n",
        "    unet         = UNet3DConditionModel.from_pretrained_2d(args.pretrained_model_path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(inference_config.unet_additional_kwargs))\n",
        "\n",
        "    if is_xformers_available(): unet.enable_xformers_memory_efficient_attention()\n",
        "    else: assert False\n",
        "\n",
        "    pipeline = AnimationPipeline(\n",
        "        vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,\n",
        "        scheduler=DDIMScheduler(**OmegaConf.to_container(inference_config.noise_scheduler_kwargs)),\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # 1. unet ckpt\n",
        "    # 1.1 motion module\n",
        "    motion_module_state_dict = torch.load(motion_module, map_location=\"cpu\")\n",
        "    if \"global_step\" in motion_module_state_dict: func_args.update({\"global_step\": motion_module_state_dict[\"global_step\"]})\n",
        "    missing, unexpected = pipeline.unet.load_state_dict(motion_module_state_dict, strict=False)\n",
        "    assert len(unexpected) == 0\n",
        "\n",
        "    # 1.2 T2I\n",
        "    if model_config.path != \"\":\n",
        "      if model_config.path.endswith(\".ckpt\"):\n",
        "        state_dict = torch.load(model_config.path)\n",
        "        pipeline.unet.load_state_dict(state_dict)\n",
        "\n",
        "      elif model_config.path.endswith(\".safetensors\"):\n",
        "        state_dict = {}\n",
        "        if T2I_TYPE == \"5-RealisticVision\":\n",
        "          model_config.path = './models/DreamBooth_LoRA/realisticVisionV40_v20Novae.safetensors'\n",
        "        with safe_open(model_config.path, framework=\"pt\", device=\"cpu\") as f:\n",
        "          for key in f.keys():\n",
        "            state_dict[key] = f.get_tensor(key)\n",
        "\n",
        "        is_lora = all(\"lora\" in k for k in state_dict.keys())\n",
        "        if not is_lora:\n",
        "          base_state_dict = state_dict\n",
        "        else:\n",
        "          base_state_dict = {}\n",
        "          with safe_open(model_config.base, framework=\"pt\", device=\"cpu\") as f:\n",
        "            for key in f.keys():\n",
        "              base_state_dict[key] = f.get_tensor(key)\n",
        "\n",
        "        # vae\n",
        "        converted_vae_checkpoint = convert_ldm_vae_checkpoint(base_state_dict, pipeline.vae.config)\n",
        "        pipeline.vae.load_state_dict(converted_vae_checkpoint)\n",
        "        # unet\n",
        "        converted_unet_checkpoint = convert_ldm_unet_checkpoint(base_state_dict, pipeline.unet.config)\n",
        "        pipeline.unet.load_state_dict(converted_unet_checkpoint, strict=False)\n",
        "        # text_model\n",
        "        pipeline.text_encoder = convert_ldm_clip_checkpoint(base_state_dict)\n",
        "\n",
        "        # import pdb\n",
        "        # pdb.set_trace()\n",
        "        if is_lora:\n",
        "          pipeline = convert_lora(pipeline, state_dict, alpha=model_config.lora_alpha)\n",
        "\n",
        "    pipeline.to(\"cuda\")\n",
        "    ### <<< create validation pipeline <<< ###\n",
        "\n",
        "    config[config_key].random_seed = []\n",
        "\n",
        "    prompt, n_prompt, random_seed = args.prompt, args.n_prompt, args.random_seed\n",
        "\n",
        "    # manually set random seed for reproduction\n",
        "    if random_seed != -1: torch.manual_seed(random_seed)\n",
        "    else: torch.seed()\n",
        "    config[config_key].random_seed.append(torch.initial_seed())\n",
        "\n",
        "    print(f\"current seed: {torch.initial_seed()}\")\n",
        "    print(f\"sampling {prompt} ...\")\n",
        "\n",
        "    sample = pipeline(\n",
        "        prompt,\n",
        "        negative_prompt     = n_prompt,\n",
        "        num_inference_steps = model_config.steps,\n",
        "        guidance_scale      = model_config.guidance_scale,\n",
        "        width               = args.W,\n",
        "        height              = args.H,\n",
        "        video_length        = args.L).videos\n",
        "    samples.append(sample)\n",
        "\n",
        "    prompt = \"-\".join((prompt.replace(\"/\", \"\").split(\" \")[:10]))\n",
        "    save_videos_grid(sample, f\"{savedir}/sample/{sample_idx}-{prompt}.gif\")\n",
        "    print(f\"save to {savedir}/sample/{prompt}.gif\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDtUQ8VXRp0f"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "Image(open(f\"{savedir}/sample/{sample_idx}-{prompt}.gif\", 'rb').read())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "69158ccfe43d0b962045f592ede11796dd42f250837ab62152c8bc6cd100a15b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
